\documentclass[11pt,a4paper]{ctexart}
%以下为所使用的宏包
\usepackage{ulem}%下划线
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsbsy}%数学符号
\usepackage{graphicx}%插入图片
\usepackage{booktabs}%三线表
%\usepackage{indentfirst}%首行缩进
\usepackage{tikz}%作图
\usepackage{appendix}%附录
\usepackage{array}%多行公式/数组
\usepackage{makecell}%表格缩并
\usepackage{siunitx}%SI单位--\SI{number}{unit}
\usepackage{mathrsfs}%数学字体
\usepackage{enumitem}%列表间距
\usepackage{multirow}%列表横向合并单元格
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}%超链接引用
\usepackage{float}%图片、表格位置排版
\usepackage{pict2e,keyval,fp,diagbox}%带有斜线的表格
\usepackage{fancyvrb,listings}%设置代码插入环境
\usepackage{minted}%代码环境设置
\usepackage{fontspec}%字体设置
\usepackage{color,xcolor}%颜色设置
\usepackage{titlesec} %自定义标题格式
\usepackage{tabularx}%列表扩展
\usepackage{authblk}%titlepage作者信息
\usepackage{nicematrix}%更好的矩阵标定
\usepackage{fbox}%更多浮动体盒子



%以下是页边距设置
\usepackage[left=0.5in,right=0.5in,top=0.81in,bottom=0.8in]{geometry}

%以下是段行设置
\linespread{1.4}%行距
\setlength{\parskip}{0.1\baselineskip}%段距
\setlength{\parindent}{2em}%缩进


%其他设置
\numberwithin{equation}{section}%公式按照章节编号
\newenvironment{point}{\raggedright$\blacktriangleright$}{}
\newenvironment{algorithm}[1]{\vspace{12pt} \hrule\hrule \vspace{3pt} \noindent\textbf{\color[HTML]{E63F00}Algorithm } \,\textit{#1} \vspace{3pt} \hrule\vspace{6pt}}{\vspace{6pt}\hrule\hrule \vspace{12pt}} % 算法伪代码格式环境


%代码环境\lst设置
\definecolor{CodeBlue}{HTML}{268BD2}
\definecolor{CodeBlue2}{HTML}{0000CD}
\definecolor{CodeGreen}{HTML}{2AA1A2}
\definecolor{CodeRed}{HTML}{CB4B16}
\definecolor{CodeYellow}{HTML}{B58900}
\definecolor{CodePurPle}{HTML}{D33682}
\definecolor{CodeGreen2}{HTML}{859900}
\lstset{
    basicstyle=\tt,%字体设置
    numbers=left, %设置行号位置
    numberstyle=\tiny\color{black}, %设置行号大小
    keywordstyle=\color{black}, %设置关键字颜色
    stringstyle=\color{CodeRed}, %设置字符串颜色
    commentstyle=\color{CodeGreen}, %设置注释颜色
    frame=single, %设置边框格式
    escapeinside=`, %逃逸字符(1左面的键)，用于显示中文
    %breaklines, %自动折行
    extendedchars=false, %解决代码跨页时，章节标题，页眉等汉字不显示的问题
    xleftmargin=2em,xrightmargin=2em, aboveskip=1em, %设置边距
    tabsize=4, %设置tab空格数
    showspaces=false, %不显示空格
    emph={TRUE,FALSE,NULL,NAN,NA,<-,},emphstyle=\color{CodeBlue2}, %其他高亮}
}


%节标题格式设置
\titleformat{\section}[block]{\large\bfseries}{Exercise \arabic{section}}{1em}{}[]
\titleformat{\subsection}[block]{}{    \arabic{section}.(\alph{subsection})}{1em}{}[]
% \titleformat{\subsubsection}[block]{\normalsize\bfseries}{    \arabic{subsection}-\alph{subsubsection}}{1em}{}[]
% \titleformat{\paragraph}[block]{\small\bfseries}{[\arabic{paragraph}]}{1em}{}[]


% \titleformat{\sectioncommand}[shape]{format}{title-label}{sep}{before-title}[after-title]



% 中文字号
% 初号42pt, 小初36pt, 一号26pt, 小一24pt, 二号22pt, 小二18pt, 三号16pt, 小三15pt, 四号14pt, 小四12pt, 五号10.5pt, 小五9pt


\begin{document}

\begin{center}\thispagestyle{plain}

{\LARGE\textbf{Stat450 - 2025 Winter}}

{\Large\textbf{HW1}}

Tuorui Peng\footnote{TuoruiPeng2028@u.northwestern.edu}
\end{center}

\thispagestyle{myheadings}\markright{Compiled using \LaTeX}
\pagestyle{myheadings}\markright{Tuorui Peng}






\section{}


\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Since $ f(x|\theta )\leq 1 $ and $ \lambda (\theta )\to 0 $ at the boundary of $ \Theta  $, we have
    \begin{align*}
        \int_\Theta \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }f(x|\theta )\lambda (\theta ) \,\mathrm{d}\theta = f(x|\theta )\lambda (\theta ) \Big|_{\text{boundary}} = 0
    \end{align*}
    \item Intergrade by parts, we have
    \begin{align*}
        \int_\Theta  \psi(\theta )\dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{f(x|\theta )\lambda (\theta )\} \,\mathrm{d}\theta = & \psi(\theta )f(x|\theta )\lambda (\theta ) \Big|_{\text{boundary}} - \int_\Theta  \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\psi(\theta )f(x|\theta )\lambda (\theta ) \,\mathrm{d}\theta \\
        =& - \int_\Theta  \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{\psi(\theta )\}f(x|\theta )\lambda (\theta ) \,\mathrm{d}\theta
    \end{align*}
    \item In the above equation, replace $ \psi(\theta ) \mapsto \hat{\psi}(x) - \psi(\theta ) $ and then add an intergral w.r.t. $ x $ on both sides, we have
    \begin{align*}
        \int_x  \int_\Theta  (\hat{\psi}(x)-\psi(\theta ))\dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{f(x|\theta )\lambda (\theta )\} \,\mathrm{d}\theta \,\mathrm{d}x = & \int_x\int_\Theta  \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{\psi(\theta )\}f(x|\theta )\lambda (\theta ) \,\mathrm{d}\theta\,\mathrm{d}x\\
        =&\int_\Theta  \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{\psi(\theta )\}\lambda (\theta ) \,\mathrm{d}\theta
    \end{align*}
    \item By cauchy-schwarz inequality, we have
    \begin{align*}
        &\mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\psi(\theta ))^2 |\theta  \right]  \right] \big(\mathbb{E}_\lambda \left[ \mathcal{I}(\theta ) \right] + \mathcal{I}(\lambda ) \big) \\
        =& \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\psi(\theta ))^2 |\theta  \right]  \right]\mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ \mathcal{I}(\theta ) + \mathcal{I}(\lambda )|\theta  \right]  \right] \\
        =&\mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\psi(\theta ))^2 |\theta  \right]  \right] \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ \left(\dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\log f(X|\theta)\right)^2 + \left(\dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\log \lambda (\theta )\right)^2|\theta  \right]  \right] \\
        \geq & \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\psi(\theta ))\left( \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\log f(X|\theta) + \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\log \lambda (\theta ) \right) |\theta  \right]  \right]^2\\
        =& \left(\int_x  \int_\Theta  (\hat{\psi}(x)-\psi(\theta ))\dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{f(x|\theta )\lambda (\theta )\} \,\mathrm{d}\theta \,\mathrm{d}x\right)^2\\
        =& \left(\int_\Theta  \dfrac{\mathrm{d}^{}  }{\mathrm{d} \theta ^{} }\{\psi(\theta )\}\lambda (\theta ) \,\mathrm{d}\theta\right)^2
    \end{align*}
    i.e. we have
    \begin{align*}
        \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\psi(\theta ))^2 |\theta  \right]  \right] \geq& \dfrac{ (\mathbb{E}_\lambda \left[ \frac{\mathrm{d}^{}  }{\mathrm{d}\theta  ^{} }\psi(\theta ) \right] )^2 }{ \mathbb{E}_\lambda \left[ \mathcal{I}(\theta ) \right] + \mathcal{I}(\lambda ) } 
    \end{align*}

    
\end{itemize}

    

\subsection{}


For Normal distribution with known variance $ 1 $, we have
\begin{align*}
    \mathcal{I}(\theta )=& \dfrac{ n }{ \sigma ^2 }  =  n\qquad \mathcal{I}(\lambda )=\mathcal{I}(\lambda _0)a^{-2}
\end{align*}
then we have using Trees' inequality:
\begin{align*}
    \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\theta ^\alpha )^2 |\theta  \right]  \right] \geq&\dfrac{ \left( \int a^{-1}\lambda _0(a^{-1}\theta )\alpha \theta ^{\alpha -1}\,\mathrm{d}\theta  \right)^2 }{ n + \mathcal{I}(\lambda _0)a^{-2} }\\
    =&\dfrac{ Aa^{2(\alpha -1)}\alpha ^2 }{ n + \mathcal{I}(\lambda _0)a^{-2} } :=  \mathrm{R.H.S.}
\end{align*}
which is true for any $ a $ so we optimize it over $ a $:

\begin{align*}
     \mathrm{R.H.S.}=& \exp\left[ \mathrm{const}+2(\alpha -1)\log a - \log(n+\mathcal{I}(\lambda _0)a^{-2}) \right]\\
    \dfrac{\partial^{} 2(\alpha -1)\log a - \log(n+\mathcal{I}(\lambda _0)a^{-2})  }{\partial a^{} }=& \dfrac{ 2(\alpha -1) }{ a } + \dfrac{ \mathcal{I}(\lambda _0)2a^{-3} }{ n+\mathcal{I}(\lambda _0)a^{-2} } = 0\\
     \Rightarrow a=&\sqrt{\dfrac{ \alpha  }{ 1-\alpha  } \dfrac{ \mathcal{I}(\lambda _0) }{ n } }
\end{align*}
thus we get the optimal $ a $ and the optimal bound:
\begin{align*}
    \mathfrak{M}\geq \mathbb{E}_\lambda \left[ \mathbb{E}_\theta \left[ (\hat{\psi}(X)-\theta ^\alpha )^2 |\theta  \right]  \right] \geq&A(1-\alpha )\alpha ^{\alpha +2}\left(\dfrac{ \mathcal{I}(\lambda _0) }{ 1-\alpha  } \right)^{\alpha -1}n^{-\alpha } \asymp n^{-\alpha }
\end{align*}

\subsection{}

By Le Cam's two point method, we have
\begin{align*}
    \mathfrak{M}\geq& \mathop{ \sup }\limits_{\theta _0,\theta _1\geq 0} \dfrac{ \ell(\theta _0,\theta _1) }{ 8 }(1-d_\mathrm{ TV }(f(x|\theta _0), f(x|\theta _1)) ) \\
    \geq &  \mathop{ \sup }\limits_{\theta _0=0, \theta _1\geq 0} \dfrac{ \ell(\theta _0,\theta _1) }{ 8 }(1-d_\mathrm{ TV }(f(x|\theta _0), f(x|\theta _1)) ) \\
    \mathop{ \geqq }\limits^{(i)}  & \mathop{ \sup }\limits_{\theta _0=0, \theta _1\geq 0} \dfrac{ \ell(\theta _0,\theta _1) }{ 8 }(1-\sqrt{\dfrac{ 1 }{ 2 } \mathrm{ KL }(f(x|\theta _0)\Vert f(x|\theta _1)) }) \\ 
    \mathop{ = }\limits^{(ii)}& \mathop{ \sup }\limits_{\theta _0=0, \theta _1\geq 0} \dfrac{ \ell(\theta _0,\theta _1=\theta ) }{ 8 }(1-\dfrac{ \sqrt{n} }{ 2 }\left\Vert \theta _1-\theta _0 \right\Vert  ) \\
    =&\mathop{ \sup }\limits_{\theta \geq 0} \dfrac{ \theta ^{2\alpha } }{ 8 }\left( 1 - \dfrac{ \sqrt{n} }{ 2 }\theta  \right)\\ 
\end{align*}
in which $ (i) $ uses Pinsker's inequality and $ (ii) $ uses the fact that $ \mathrm{KL}(f(x|\theta _0)\Vert f(x|\theta _1)) = \dfrac{ 1 }{ 2 } \left\Vert \theta _1-\theta _0 \right\Vert ^2 $ for normal distribution (with the same known variance $ 1 $). Optimize the above bound w.r.t. $ \theta \geq 0 $ and we get optimal $ \theta = 2\dfrac{ 2\alpha  }{ 2\alpha +1 } \dfrac{ 1 }{ \sqrt{n} }  $, then we have the optimal bound for Le Cam's two point method:
\begin{align*}
    \mathfrak{M}\gtrsim \dfrac{ 1 }{ 2\alpha +1 } \left(\dfrac{ 2\alpha  }{ 2\alpha +1 } \right)^{2\alpha } n^{-\alpha }\asymp n^{-\alpha } 
\end{align*}
which gives the same order of convergence as the optimal bound in the previous question.


\subsection{}

We prove that $ (\bar{X}+n^{-1/2})^\alpha $ is an estimator that achieves the optimal rate of convergence. 

First for $ \bar{X} $ we have
\begin{align*}
    \bar{X} \sim \mathcal{N}(\theta ,n^{-1}):= \theta + \dfrac{ 1 }{ \sqrt{n} } \varepsilon  
\end{align*}
i.e. $ \varepsilon \sim \mathcal{N}(0,1) $.

We have the following using taylor expansion to the 2
\begin{align*}
    \mathbb{E}_\theta \left[ ((\bar{X}+n^{-1/2})^\alpha -\theta ^\alpha )^2 \right]
    =& \mathbb{E}\left[ \Big(\big( \theta + n^{-1/2} + n^{-1/2}\varepsilon  \big)^\alpha -\theta ^\alpha \Big)^2  \right]   \\  
    =& \mathbb{E}\left[ \big(  \theta^\alpha  + \alpha \theta ^{\alpha -1}(1+\varepsilon )\Theta (n^{-1/2}) + \alpha (\alpha -1)\theta ^{\alpha -2}(1+\varepsilon )^2 \Theta (n^{-1}) -\theta ^\alpha   \big)^2 \right] \\
    =&\mathbb{E}\left[ \big(   \alpha \theta ^{\alpha -1}(1+\varepsilon )\Theta (n^{-1/2}) + \alpha (\alpha -1)\theta ^{\alpha -2}(1+\varepsilon )^2 \Theta (n^{-1})   \big)^2 \right] \\
    =&\mathbb{E}\left[ \alpha ^2\theta ^{2\alpha -2}(1+\varepsilon )^2\Theta (n^{-1}) + 2\alpha ^2(\alpha -1)\theta ^{2\alpha -3}(1+\varepsilon )^3\Theta (n^{-3/2}) \right]\\
    \asymp & \theta ^{2\alpha -2}\Theta (n^{-1}) + \theta ^{2\alpha -3}\Theta (n^{-3/2})
\end{align*}
then we optimize the above bound w.r.t. $ \theta $ to get the optimual rate of $ \theta ^* \asymp n^{-1/2} $, which gives that
\begin{align*}
    \sup_{\theta \geq 0} \mathbb{E}_\theta \left[ ((\bar{X}+n^{-1/2})^\alpha -\theta ^\alpha )^2 \right] =&\mathbb{E}_{\theta ^*}\left[ ((\bar{X}+n^{-1/2})^\alpha -(\theta ^*) ^\alpha )^2 \right]\\
    \asymp & (\theta ^*)^{2\alpha -2}n^{-1} + (\theta ^*)^{2\alpha -3}n^{-3/2} \\
    =& n^{-\alpha }
\end{align*}



Thus we have proved that $ (\bar{X}+n^{-1/2})^\alpha $ is an estimator that achieves the optimal rate of convergence $ n^{-\alpha } $.






    

\section{}

\subsection{}

We have the following:
\begin{align*}
    \mathbb{E}_\theta\left[ Y \right] =& \int y \mathbb{P}_\theta (y)  \,\mathrm{d}y  \\    
    =& \int yh(y)\exp\left[ \dfrac{ y\left\langle x,\theta  \right\rangle -\Phi(\left\langle x,\theta  \right\rangle ) }{ s(\sigma ) }  \right] \,\mathrm{d}y\\
    =& \int h(y)s(\sigma ) \left[ \dfrac{\partial^{}  }{\partial \left\langle x,\theta  \right\rangle ^{} }\exp\left[ \dfrac{ y\left\langle x,\theta  \right\rangle -\Phi(\left\langle x,\theta  \right\rangle ) }{ s(\sigma ) }  \right] + \dfrac{ \Phi'(\left\langle x,\theta  \right\rangle ) }{ s(\sigma ) } \exp\left[ \dfrac{ y\left\langle x,\theta  \right\rangle -\Phi(\left\langle x,\theta  \right\rangle ) }{ s(\sigma ) }  \right] \right]  \,\mathrm{d}y\\
    =& s(\sigma )\dfrac{\partial^{}  }{\partial \left\langle x,\theta  \right\rangle ^{} }1+ \int h(y)\Phi'(\left\langle x,\theta  \right\rangle )\exp\left[ \dfrac{ y\left\langle x,\theta  \right\rangle -\Phi(\left\langle x,\theta  \right\rangle ) }{ s(\sigma ) }  \right] \,\mathrm{d}y\\
    =& \Phi'(\left\langle x,\theta  \right\rangle )
\end{align*}
then we can compute the KL divergence:
\begin{align*}
    \mathrm{ KL } (\mathbb{P}_\theta \Vert \mathbb{P}_{\theta '} ) =& \mathbb{E}_{\theta}\left[ \log \dfrac{ \mathbb{P}_{\theta } }{ \mathbb{P}_{\theta '} }  \right] \\
    =& \int \prod_{i=1}^n h(y_i)\exp\left[ \dfrac{ y_i\left\langle x_i,\theta  \right\rangle -\Phi(\left\langle x_i,\theta  \right\rangle ) }{ s(\sigma ) }  \right] \sum_{i=1}^n \dfrac{ y_i\left\langle x_i,\theta -\theta ' \right\rangle -\Phi(\left\langle x_i,\theta  \right\rangle ) + \Phi(\left\langle x_i,\theta ' \right\rangle ) }{ s(\sigma ) } \,\mathrm{d}^ny\\
    =& \sum_{i=1}^n \left( \dfrac{ \left\langle x_i,\theta -\theta ' \right\rangle  }{ s(\sigma ) } \Phi'(\left\langle x_i,\theta  \right\rangle )- \dfrac{ \Phi(\left\langle x_i,\theta  \right\rangle )-\Phi(\left\langle x_i,\theta ' \right\rangle ) }{ s(\sigma ) }  \right)\\
    =& \dfrac{ 1 }{ s(\sigma ) } \sum_{i=1}^n \left( \Phi(\left\langle x_i,\theta ' \right\rangle )  - \Phi(\left\langle x_i,\theta  \right\rangle ) - \Phi'(\left\langle x_i,\theta  \right\rangle )(\left\langle x_i,\theta '-\theta  \right\rangle ) \right)
\end{align*}


\subsection{}

Note that we have
\begin{align*}
    \Phi(\left\langle x_i,\theta ' \right\rangle )  - \Phi(\left\langle x_i,\theta  \right\rangle ) =& \int_{\left\langle x_i,\theta  \right\rangle }^{\left\langle x_i,\theta ' \right\rangle } \Phi'(t)\,\mathrm{d}t\\
    =& \int_{\left\langle x_i,\theta  \right\rangle }^{\left\langle x_i,\theta ' \right\rangle } \left(\int_{\left\langle x_i,\theta  \right\rangle }^{t} \Phi''(\tau)\,\mathrm{d}\tau + \Phi'(\left\langle x_i,\theta  \right\rangle ) \right) \,\mathrm{d}t\\
    \leq& \int_{\left\langle x_i,\theta  \right\rangle }^{\left\langle x_i,\theta ' \right\rangle } (L\left\Vert t-\left\langle x,\theta  \right\rangle   \right\Vert +  \Phi'(\left\langle x_i,\theta  \right\rangle ) ) \,\mathrm{d}t\\
    \leq& \dfrac{ L }{ 2 }\left\Vert x_i'(\theta '-\theta ) \right\Vert ^2 + \Phi'(\left\langle x_i,\theta  \right\rangle )\left\langle x_i,\theta '-\theta  \right\rangle 
\end{align*}
Thus 
\begin{align*}
    \mathrm{ KL } (\mathbb{P}_\theta \Vert \mathbb{P}_{\theta '} ) \leq &\dfrac{ 1 }{ s(\sigma ) } \sum_{i=1}^n \left( \Phi(\left\langle x_i,\theta ' \right\rangle )  - \Phi(\left\langle x_i,\theta  \right\rangle ) - \Phi'(\left\langle x_i,\theta  \right\rangle )(\left\langle x_i,\theta '-\theta  \right\rangle ) \right)\\
    \leq& \dfrac{ L }{ 2s(\sigma ) } \sum_{i=1}^n \left\Vert x_i'(\theta '-\theta ) \right\Vert^2 \\
    =& \dfrac{ L }{ 2s(\sigma ) } \left\Vert X'(\theta '-\theta ) \right\Vert^2\\
    \leq& n\dfrac{ L\eta^2_{\max} }{ 2s(\sigma ) }\left\Vert \theta -\theta '
     \right\Vert _2^2 
\end{align*}
in which $ \eta_{\max}^2 $ is the maximum singular value of $ X/\sqrt{n} $ (as define in question (c)).

\subsection{}

Using Fano's bound we have
\begin{align*}
    \mathop{ \inf }\limits_{\hat{\theta }}\mathop{ \sup }\limits_{\theta \in \mathbb{B}_d(1)} \mathbb{E}\left[ \left\Vert \hat{\theta }-\theta  \right\Vert _2^2 \right] \geq & \delta ^2\left(1-\dfrac{ I(Z;J)+\log 2 }{ \log \left\vert \mathcal{M}(2\delta ,\mathbb{B}_d(1), \left\Vert \, \cdot \,  \right\Vert _2^2 ) \right\vert  }  \right),\quad \forall \delta .
\end{align*}
We try the following: $ \delta ^2 = c\dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n }   $ in which $ c $ t.b.d. and we hope $ c $ to satisfy:
\begin{align*}
    \log \left\vert \mathcal{M}(2\delta ,\mathbb{B}_d(1), \left\Vert \, \cdot \,  \right\Vert _2^2 ) \right\vert \geq 2( I(Z;J)+\log 2)
\end{align*}
For which we make the following argument:
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item For mutual information we have bound
    \begin{align*}
        I(Z;J)\leq \dfrac{ 1 }{ M^2\sum_{j,k=1}^M }\mathrm{ KL }(\mathbb{P}_{\theta _j}\Vert \mathbb{P}_{\theta _k})\leq n\dfrac{ L\eta^2_{\max} }{ 2s(\sigma ) }(2\delta )^2 = \dfrac{ 2L\eta_{\max}^2 }{ s(\sigma ) }n\delta ^2 
    \end{align*}
    in which $ \theta _j,\theta _k\in \mathcal{M}(2\delta ) $.
    
    \item For 2$ \delta  $-packing number we have by volume argument:
    \begin{align*}
        \log \left\vert \mathcal{M}(2\delta ,\mathbb{B}_d(1), \left\Vert \, \cdot \,  \right\Vert _2^2 ) \right\vert \gtrsim & \log \left( \dfrac{ 1 }{ (2\delta )^d } \right) := Cd\log \dfrac{ 1 }{ \delta  } 
    \end{align*}
    
    \item Together we have the condition of such $ c $ being:
    \begin{align*}
        Cd\log \dfrac{ 1 }{ \delta  }\geq 2\left( \dfrac{ 2L\eta_{\max}^2 }{ s(\sigma ) }n\delta ^2  + \log 2\right) 
    \end{align*}
    substitute $ \delta ^2 = c\dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n }   $ we have
    \begin{align*}
        &\dfrac{ Cd }{ 2 }\left( -\log \dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n }  - \log c  \right)  \geq 2(2cd + \log 2)\\
        \Leftrightarrow& (\dfrac{ C }{ 2 }\log c +4c ) \leq -\dfrac{ C }{ 2 }\log \dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n }  -\dfrac{ 2 }{ d } \log 2
    \end{align*}
    in which we note that, if $ d\leq n $, the right hand side is lower bounded by a univerasal constant, thus we can always find such $ c $.
\end{itemize}
    choose such $ c $ as above and we have for such $ c $ that:
    \begin{align*}
        \mathop{ \inf }\limits_{\hat{\theta }}\mathop{ \sup }\limits_{\theta \in \mathbb{B}_d(1)} \mathbb{E}\left[ \left\Vert \hat{\theta }-\theta  \right\Vert _2^2 \right] \geq & \delta ^2\left(1-\dfrac{ I(Z;J)+\log 2 }{ \log \left\vert \mathcal{M}(2\delta ,\mathbb{B}_d(1), \left\Vert \, \cdot \,  \right\Vert _2^2 ) \right\vert  }  \right)\\
        \geq & \dfrac{ 1 }{ 2 }\delta ^2 = \dfrac{ c }{ 2 }\dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n } 
    \end{align*}
    adding the trivial lower bound $ 1 $ for $ \hat{\theta }\equiv 0 $, and we have the desired result:
    \begin{align*}
        \mathop{ \inf }\limits_{\hat{\theta }}\mathop{ \sup }\limits_{\theta \in \mathbb{B}_d(1)} \mathbb{E}\left[ \left\Vert \hat{\theta }-\theta  \right\Vert _2^2 \right] \geq & \min \big\{ 1, c\dfrac{ s(\sigma ) }{ L\eta_{\max}^2 }\dfrac{ d }{ n }  \big\}.
    \end{align*}


\subsection{}

We have the following:
\begin{align*}
    \mathop{ \inf }\limits_{\hat{\theta }}\mathop{ \sup }\limits_{\theta \in \mathbb{B}_d(1)}\mathbb{E}\left[ \left\Vert X(\hat{\theta }-\theta)  \right\Vert _2^2 \right] \geq & \eta_{\min}^2\mathop{ \inf }\limits_{\hat{\theta }}\mathop{ \sup }\limits_{\theta \in \mathbb{B}_d(1)} \mathbb{E}\left[ \left\Vert \hat{\theta }-\theta  \right\Vert _2^2 \right] \\
    \geq& c\dfrac{ s(\sigma )\eta_{\min}^2 }{ L\eta_{\max}^2 }\dfrac{ d }{ n } \\
    \gtrsim &s(\sigma )\dfrac{ d }{ n } 
\end{align*}
which gives the same rate as Linear regression $ s(\sigma )=\sigma ^2 $, which is an example in MJW page 504.


    
    
    
       













\section{}

We use the Heillinger distance, which has the following properties:
\begin{align*}
    H^2(P,Q):=&\int(\sqrt{p}-\sqrt{q})^2\,\mathrm{d}\mu\\
    1-\dfrac{ 1 }{ 2 } H^2(P,Q)\mathop{ = }\limits^{(i)} & \int \sqrt{pq}\,\mathrm{d}\mu\\
    d_\mathrm{ TV }(P,Q)\leq& H(P,Q)\sqrt{1-\dfrac{ H^2(P,Q) }{ 4 } }
\end{align*}
then we construct the following:
\begin{enumerate}[topsep=2pt,itemsep=2pt]
    \item 
    \begin{align*}
        \mathop{ \max }\limits_{j=0,1}P_j(\psi\neq j)\geq & \dfrac{ 1 }{ 2 } \left(P_0(\psi\neq 0)+P_1(\psi\neq 1)\right)\\
        \geq & \dfrac{ 1 }{ 2 } \left( 1- d_\mathrm{ TV }(P_0,P_1) \right)\\
        \geq& \dfrac{ 1 }{ 2 } \left( 1- H(P_0,P_1)\sqrt{1-\dfrac{ H^2(P_0,P_1) }{ 4 } } \right)  
    \end{align*}
    \item 
    \begin{align*}
         \left(\int \sqrt{pq}\,\mathrm{d}\mu\right)^2 =& \exp 2\log\left( \int \sqrt{pq}\,\mathrm{d}\mu \right)\\
         =& \exp 2\log \int \dfrac{ \sqrt{p} }{ \sqrt{q} } q\,\mathrm{d}\mu\\
         =& \exp 2\log \mathbb{E}_Q\left[ \sqrt{p}/\sqrt{q} \right] \\
         \geq & \exp 2\mathbb{E}_Q\left[ \log \sqrt{p}/\sqrt{q} \right] \\
         =& \exp\left[ -\mathbb{E}_Q\left[ \log p/q \right]  \right]\\
         =& \exp\left[ -\mathrm{KL}(Q,P) \right]
    \end{align*}
    and with $ Q\mapsto P_0$, $ P\mapsto P_1 $ we have
    \begin{align*}
        \left(\int \sqrt{\,\mathrm{d}P_0 \,\mathrm{d}P_1}\,\mathrm{d}\mu\right)^2 \mathop{ \geq }\limits^{(ii)}  \exp\left[ -\mathrm{ KL }(P_1,P_0) \right]
    \end{align*}
    
    \item Using the relation $ 1-\frac{1}{2}H^2(P,Q)=\int \sqrt{pq}\,\mathrm{d}\mu $, it suffices to show the following $ \mathop{ \geq }\limits^{(?)}  $:
    \begin{align*}
        2-2H(P_0,P_1)\sqrt{1-\dfrac{ H^2(P_0,P_1) }{ 4 } }\mathop{ \geq  }\limits^{(?)} \left(1-\dfrac{ 1 }{ 2 } H^2(P_0,P_1)\right)^2\mathop{ = }\limits^{(i)}     
        \left(\int \sqrt{\,\mathrm{d}P_0 \,\mathrm{d}P_1}\,\mathrm{d}\mu\right)^2  \mathop{ \geq }\limits^{(ii)} \exp\left[ -\mathrm{ KL }(P_1,P_0) \right]
    \end{align*}
    in which denote $ H(P_0,P_1)= 2\cos \theta $, then we have
    \begin{align*}
        &2-2H(P_0,P_1)\sqrt{1-\dfrac{ H^2(P_0,P_1) }{ 4 } }\mathop{ \geq  }\limits^{(?)} \left(1-\dfrac{ 1 }{ 2 } H^2(P_0,P_1)\right)^2\\
        \Leftrightarrow& 2-4\cos\theta \sin\theta  \geq (1-2\cos\theta )^2\\
        \Leftrightarrow& 2-2\sin 2\theta \geq 1-\sin^2 2\theta \\
        \Leftrightarrow& \sin^2 2\theta -2\sin 2\theta  + 1 \geq 0\\
        \Leftrightarrow& (\sin 2\theta -1)^2 \geq 0
    \end{align*}
    which is always true, thus we prove that 
    \begin{align*}
        \mathop{ \max }\limits_{j=0,1}P_j(\psi\neq j)\geq\exp\left[ -\mathrm{ KL } (P_1,P_0) \right]
    \end{align*}
    
    
    
    
    
    
    
    
    
    
    
\end{enumerate}

    


 
 
















\end{document}